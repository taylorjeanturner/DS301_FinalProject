---
title: "hw10 - Kordell Schrock"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(leaps)
library(caret)
library(ISLR2)
library(glmnet)
library(boot)
library (plyr)
library(MASS)
library(class)
library(heplots)
library(ggplot2)
library(dplyr)
library(gridExtra)
df = read.csv('./heart_2020_cleaned.csv')
df$HeartDisease <- ifelse(df$HeartDisease == 'Yes', 1, 0)
df$Smoking = factor(df$Smoking)
df$AlcoholDrinking = factor(df$AlcoholDrinking)
df$Stroke = factor(df$Stroke)
df$DiffWalking = factor(df$DiffWalking)
df$Sex = factor(df$Sex)
df$AgeCategory = factor(df$AgeCategory)
df$Race = factor(df$Race)
df$Diabetic = factor(df$Diabetic)
df$PhysicalActivity = factor(df$PhysicalActivity)
df$GenHealth = factor(df$GenHealth)
df$Asthma = factor(df$Asthma)
df$KidneyDisease = factor(df$KidneyDisease)
df$SkinCancer = factor(df$SkinCancer)
```
# Lasso Regression 1:
#### Use 10-fold CV to obtain the optimal λ. Present a plot of the cross-validation error as a function of λ. Report that value here

#### Prediction: Given other predictors like age, kidney disease, skin cancer, heart disease, etc, can we predict the BMI of a person?
##### here the model finds that HeartDisease is the best predictor in predicting BMI
```{r}
set.seed(1)
grid <- 10^seq(10,-2,length= 100)
train <- sample(1:nrow(df), nrow(df)/2) 
test <- (-train)
y = df$BMI
y.test <-  y[test]
               
x = model.matrix(BMI ~.,df)[,-1] 
y = df[,1] 

lassoo.train = glmnet(x[train,],y[train],alpha=1,lambda=grid)
cv.out <- cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.out)
bestlambda = cv.out$lambda.min
bestlambda
log(bestlambda) #confirm min aligns with min on plot
lasso.pred = predict(lassoo.train,s=bestlambda,newx=x[test,])
mean((lasso.pred-y.test)^2)

final.lasso = glmnet(x,y,alpha=1,lambda=bestlambda)
coef(final.lasso)

```
# Lasso Regression 2:
#### Prediction: Given other predictors like age, kidney disease, skin cancer, heart disease, etc, can we predict the BMI of a person? REMOVING heart disease predictor
##### removed predictor HeartDisease, to find another model
```{r}
set.seed(1)
grid = 10^seq(10,-2,length=100)
train <- sample(1:nrow(df), nrow(df)/2) 
test <- (-train)
y = df$BMI
y.test <-  y[test]
               
x = model.matrix(BMI ~.,df)[,-1] 
x = x[,-1] # THIS removes HeartDisease predictor
y = df[,1] 


cv.out <- cv.glmnet(x[train,], y[train], alpha=1 ,lambda=grid)
#default performs 10-fold CV, but you can change this using the argument `nfolds` 
plot(cv.out)
bestlambda = cv.out$lambda.min
bestlambda
log(bestlambda) #confirm min aligns with min on plot
lasso.train = glmnet(x[train,],y[train],alpha=1,lambda=grid)

which(grid==bestlambda)
lasso.train$lambda[100]
coef(lasso.train)[,100]

lasso.train.pred = predict(lasso.train,s=bestlambda,newx=x[train,])# on training set only
lasso.pred = predict(lasso.train,s=bestlambda,newx=x[test,]) # evaluating on test set
mean((lasso.pred-y.test)^2) # test MSE

final.lasso = glmnet(x,y,alpha=1,lambda=bestlambda) # fitting on entire data set using optimal lamda
coef(final.lasso)

```



# Ridge Regression:

```{r}
set.seed(1)
grid = 10^seq(10,-2,length=100)
train <- sample(1:nrow(df), nrow(df)/2) 
test <- (-train)
y = df$BMI
y.test <-  y[test]
               
x = model.matrix(BMI ~.,df)[,-1] 
y = df[,1] 

ridge.train = glmnet(x[train,],y[train],alpha=0,lambda=grid) # on training set, alpha 0 for ridge regression

cv.out = cv.glmnet(x[train,],y[train],alpha = 0, lambda = grid) 
#default performs 10-fold CV, but you can change this using the argument `nfolds` 
plot(cv.out)
bestlambda = cv.out$lambda.min
bestlambda

## what is the test MSE associated with this value of lambda? 
### glmnet() has its own predict() function! This is different than the predict() 
## we are use to using, so the syntax is slightly different. 

ridge.pred = predict(ridge.train,s=bestlambda,newx=x[test,])
mean((ridge.pred-Y.test)^2)

## We can refit the ridge regression model on the full data set, using the value 
## of lambda chosen by cross-validation: 

final = glmnet(x,y,alpha=0,lambda = bestlambda)
coef(final)

```
```{r}
# Compute R^2 from true and predicted values
eval_results <- function(true, predicted, df) {
  SSE <- sum((predicted - true)^2)
  SST <- sum((true - mean(true))^2)
  R_square <- 1 - SSE / SST
  RMSE = sqrt(SSE/nrow(df))

  
  # Model performance metrics
data.frame(
  RMSE = RMSE,
  Rsquare = R_square
)
  
}

# Prediction and evaluation on train data
predictions_train <- predict(ride.cv.out, s = optimal_lambda, newx = x)
eval_results(y[train], predictions_train, df[train])

# Prediction and evaluation on test data
predictions_test <- predict(ridge_reg, s = optimal_lambda, newx = x_test)
eval_results(y[test], predictions_test, df[test])
```

